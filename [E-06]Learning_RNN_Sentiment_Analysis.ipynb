{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sweet-mongolia",
   "metadata": {},
   "source": [
    "## # 학습목표\n",
    "- 텍스트 데이터를 머신러닝 입출력용 수치데이터로 변환하는 과정을 이해한다.\n",
    "- RNN의 특징을 이해하고 시퀀셜한 데이터를 다루는 방법을 이해한다.\n",
    "- 1-D CNN으로도 텍스트를 처리할 수 있음을 이해한다.\n",
    "- IMDB와 네이버 영화리뷰 데이터셋을 이용한 영화리뷰 감성 분류 실습을 진행한다.\n",
    "\n",
    "(https://dbr.donga.com/article/view/1202/article_no/8891/ac/magazine) 링크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-dispatch",
   "metadata": {},
   "source": [
    "### # 텍스트 데이터의 특징\n",
    "- 텍스트를 어떻게 숫자로 나타낼 것인가..\n",
    "- 텍스트는 입력의 순서가 중요한데 인공지능 모델에게는 어떻게 적용시켜야 할까..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-patient",
   "metadata": {},
   "source": [
    "## # 텍스트를 숫자로 표현하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comfortable-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "emerging-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# 위와 같은 식으로 문장을 단어로 쪼개서 딕셔너리 구조로 만들어보기\n",
    "\n",
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-andrews",
   "metadata": {},
   "source": [
    "> 그런데 `텍스트:숫자` 구조가 되어야 하니 순서 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extreme-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exclusive-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-lover",
   "metadata": {},
   "source": [
    "> 텍스트를 숫자로 한 번 바꿔서 표현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protecting-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "institutional-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "played-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pregnant-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DJ's practice\n",
    "def decode(encoded_list, word_to_index):\n",
    "    new_sentence = []\n",
    "    for ind in encoded_list:\n",
    "        for value, index in word_to_index.items():\n",
    "            if ind == 1:\n",
    "                continue\n",
    "            if index == ind:\n",
    "                new_sentence.append(value)\n",
    "    result = ' '.join(new_sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "threaded-perfume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i feel hungry'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([1, 3, 4, 5], word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "undefined-patrick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7f3c8d611d50>\n"
     ]
    }
   ],
   "source": [
    "print(i if i % 2 == 0 else i+10 for i in [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "younger-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-submission",
   "metadata": {},
   "source": [
    "## # 임베딩 레이어(Embedding Layer)\n",
    "\n",
    "> 지금의 'i feel hungry' -> 1, 3, 4, 5 매치는 의미 없는 숫자와의 연결임  \n",
    "> 그래서 의미가 있는 벡터를 만들고 그것을 학습하는 딥러닝 모델을 만드는 과정임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "protecting-michael",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-71cdd4599bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mraw_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m     if any(isinstance(x, (\n\u001b[1;32m    959\u001b[0m         np_arrays.ndarray, np.ndarray, float, int)) for x in input_list):\n\u001b[0;32m--> 960\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_convert_numpy_or_python_types\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3310\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-pendant",
   "metadata": {},
   "source": [
    "> Embedding 레이어의 인풋 문장은 길이가 다 같아야 한다.  \n",
    "> 여기서 aw_inputs의 3개 벡터의 길이는 각각 4, 4, 5  \n",
    "> 그래서 padding이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "olympic-cannon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 3, 4, 5]), list([1, 3, 6, 7]), list([1, 8, 3, 4, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "systematic-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "special-hacker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.04694314  0.0040275   0.03234733 -0.00532321]\n",
      "  [ 0.01458072  0.01792058  0.02315973  0.01939174]\n",
      "  [-0.01427897 -0.00683196 -0.00556726  0.03361437]\n",
      "  [ 0.01303739 -0.00495904 -0.0119462  -0.04464803]\n",
      "  [-0.00979844 -0.01037434  0.04583276  0.04448194]]\n",
      "\n",
      " [[ 0.04694314  0.0040275   0.03234733 -0.00532321]\n",
      "  [ 0.01458072  0.01792058  0.02315973  0.01939174]\n",
      "  [ 0.04082792 -0.03938426 -0.00587777 -0.0490593 ]\n",
      "  [-0.02973906 -0.0464872   0.02980584  0.02103685]\n",
      "  [-0.00979844 -0.01037434  0.04583276  0.04448194]]\n",
      "\n",
      " [[ 0.04694314  0.0040275   0.03234733 -0.00532321]\n",
      "  [ 0.00316907  0.02588257  0.02558431 -0.04665069]\n",
      "  [ 0.01458072  0.01792058  0.02315973  0.01939174]\n",
      "  [-0.01427897 -0.00683196 -0.00556726  0.03361437]\n",
      "  [ 0.03727177 -0.04389333  0.03388802  0.01123264]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 위에 오류난 것을 padding 추가 후 다시 시도해보기\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-hydrogen",
   "metadata": {},
   "source": [
    "## # RNN\n",
    "> 시퀀스 데이터의 개념 파악  \n",
    "> 시퀀스 데이터는 입력이 시간의 축을 따라 발생하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "auburn-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델을 사용하여 이전 스텝의 텍스트 데이터를 처리하는 예제 코드를 구현\n",
    "\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-rental",
   "metadata": {},
   "source": [
    "### # 꼭 RNN이어야 할까?\n",
    "> 답을 하자면, 1-D Convolution Neural Network(1-D CNN)를 사용할 수도 있다.  \n",
    "> 이미지는 2-D CNN을 했는데, 문장은 1차원(단어를 여러 요소로 갖는)으로 해서 넣을 수도 있다.  \n",
    "> 이러면 그 벡터를 스캐닝하면서 특징을 추출한다.\n",
    "> 그리고 병렬처리에 RNN보다 더 효율적이라 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "productive-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN으로 만든 모델 구현\n",
    "\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-feature",
   "metadata": {},
   "source": [
    "#### 아주 간단히는 Global Average Pooling layer만 놓아도 됨\n",
    "> 이 방식은 전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출하여 그것으로 문장의 긍정/부정을 평가하는 방식이라고 생각할 수 있는데, 의외로 성능이 잘 나올 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alert-orchestra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-green",
   "metadata": {},
   "source": [
    "## # IMDB 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-scott",
   "metadata": {},
   "source": [
    "> `.load_data()`에 `num_words=`로 주는 숫자만큼 most frequent하게 등장하는 애들만 word_to_index 딕셔너리 같은 사전에 담겨서 데이터를 생성한다.  \n",
    "> 나머지는 `ovv_char`로 나타난단다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "integral-brush",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 데이터셋으로 50000개가 있는데\n",
    "# 25000개는 train, 25000개는 test로 사용하도록 올려놓음\n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "significant-constitution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "# 실제 데이터 확인\n",
    "\n",
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-israel",
   "metadata": {},
   "source": [
    "> 이미 숫자로 encode되어 있다.  \n",
    "> 데이터셋에 담긴 단어사전도 한 번 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interested-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# .get_word_index()로 가져오기\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "medical-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34701: 'fawn',\n",
       " 52006: 'tsukino',\n",
       " 52007: 'nunnery',\n",
       " 16816: 'sonja',\n",
       " 63951: 'vani',\n",
       " 1408: 'woods',\n",
       " 16115: 'spiders',\n",
       " 2345: 'hanging',\n",
       " 2289: 'woody',\n",
       " 52008: 'trawling',\n",
       " 52009: \"hold's\",\n",
       " 11307: 'comically',\n",
       " 40830: 'localized',\n",
       " 30568: 'disobeying',\n",
       " 52010: \"'royale\",\n",
       " 40831: \"harpo's\",\n",
       " 52011: 'canet',\n",
       " 19313: 'aileen',\n",
       " 52012: 'acurately',\n",
       " 52013: \"diplomat's\",\n",
       " 25242: 'rickman',\n",
       " 6746: 'arranged',\n",
       " 52014: 'rumbustious',\n",
       " 52015: 'familiarness',\n",
       " 52016: \"spider'\",\n",
       " 68804: 'hahahah',\n",
       " 52017: \"wood'\",\n",
       " 40833: 'transvestism',\n",
       " 34702: \"hangin'\",\n",
       " 2338: 'bringing',\n",
       " 40834: 'seamier',\n",
       " 34703: 'wooded',\n",
       " 52018: 'bravora',\n",
       " 16817: 'grueling',\n",
       " 1636: 'wooden',\n",
       " 16818: 'wednesday',\n",
       " 52019: \"'prix\",\n",
       " 34704: 'altagracia',\n",
       " 52020: 'circuitry',\n",
       " 11585: 'crotch',\n",
       " 57766: 'busybody',\n",
       " 52021: \"tart'n'tangy\",\n",
       " 14129: 'burgade',\n",
       " 52023: 'thrace',\n",
       " 11038: \"tom's\",\n",
       " 52025: 'snuggles',\n",
       " 29114: 'francesco',\n",
       " 52027: 'complainers',\n",
       " 52125: 'templarios',\n",
       " 40835: '272',\n",
       " 52028: '273',\n",
       " 52130: 'zaniacs',\n",
       " 34706: '275',\n",
       " 27631: 'consenting',\n",
       " 40836: 'snuggled',\n",
       " 15492: 'inanimate',\n",
       " 52030: 'uality',\n",
       " 11926: 'bronte',\n",
       " 4010: 'errors',\n",
       " 3230: 'dialogs',\n",
       " 52031: \"yomada's\",\n",
       " 34707: \"madman's\",\n",
       " 30585: 'dialoge',\n",
       " 52033: 'usenet',\n",
       " 40837: 'videodrome',\n",
       " 26338: \"kid'\",\n",
       " 52034: 'pawed',\n",
       " 30569: \"'girlfriend'\",\n",
       " 52035: \"'pleasure\",\n",
       " 52036: \"'reloaded'\",\n",
       " 40839: \"kazakos'\",\n",
       " 52037: 'rocque',\n",
       " 52038: 'mailings',\n",
       " 11927: 'brainwashed',\n",
       " 16819: 'mcanally',\n",
       " 52039: \"tom''\",\n",
       " 25243: 'kurupt',\n",
       " 21905: 'affiliated',\n",
       " 52040: 'babaganoosh',\n",
       " 40840: \"noe's\",\n",
       " 40841: 'quart',\n",
       " 359: 'kids',\n",
       " 5034: 'uplifting',\n",
       " 7093: 'controversy',\n",
       " 21906: 'kida',\n",
       " 23379: 'kidd',\n",
       " 52041: \"error'\",\n",
       " 52042: 'neurologist',\n",
       " 18510: 'spotty',\n",
       " 30570: 'cobblers',\n",
       " 9878: 'projection',\n",
       " 40842: 'fastforwarding',\n",
       " 52043: 'sters',\n",
       " 52044: \"eggar's\",\n",
       " 52045: 'etherything',\n",
       " 40843: 'gateshead',\n",
       " 34708: 'airball',\n",
       " 25244: 'unsinkable',\n",
       " 7180: 'stern',\n",
       " 52046: \"cervi's\",\n",
       " 40844: 'dnd',\n",
       " 11586: 'dna',\n",
       " 20598: 'insecurity',\n",
       " 52047: \"'reboot'\",\n",
       " 11037: 'trelkovsky',\n",
       " 52048: 'jaekel',\n",
       " 52049: 'sidebars',\n",
       " 52050: \"sforza's\",\n",
       " 17633: 'distortions',\n",
       " 52051: 'mutinies',\n",
       " 30602: 'sermons',\n",
       " 40846: '7ft',\n",
       " 52052: 'boobage',\n",
       " 52053: \"o'bannon's\",\n",
       " 23380: 'populations',\n",
       " 52054: 'chulak',\n",
       " 27633: 'mesmerize',\n",
       " 52055: 'quinnell',\n",
       " 10307: 'yahoo',\n",
       " 52057: 'meteorologist',\n",
       " 42577: 'beswick',\n",
       " 15493: 'boorman',\n",
       " 40847: 'voicework',\n",
       " 52058: \"ster'\",\n",
       " 22922: 'blustering',\n",
       " 52059: 'hj',\n",
       " 27634: 'intake',\n",
       " 5621: 'morally',\n",
       " 40849: 'jumbling',\n",
       " 52060: 'bowersock',\n",
       " 52061: \"'porky's'\",\n",
       " 16821: 'gershon',\n",
       " 40850: 'ludicrosity',\n",
       " 52062: 'coprophilia',\n",
       " 40851: 'expressively',\n",
       " 19500: \"india's\",\n",
       " 34710: \"post's\",\n",
       " 52063: 'wana',\n",
       " 5283: 'wang',\n",
       " 30571: 'wand',\n",
       " 25245: 'wane',\n",
       " 52321: 'edgeways',\n",
       " 34711: 'titanium',\n",
       " 40852: 'pinta',\n",
       " 178: 'want',\n",
       " 30572: 'pinto',\n",
       " 52065: 'whoopdedoodles',\n",
       " 21908: 'tchaikovsky',\n",
       " 2103: 'travel',\n",
       " 52066: \"'victory'\",\n",
       " 11928: 'copious',\n",
       " 22433: 'gouge',\n",
       " 52067: \"chapters'\",\n",
       " 6702: 'barbra',\n",
       " 30573: 'uselessness',\n",
       " 52068: \"wan'\",\n",
       " 27635: 'assimilated',\n",
       " 16116: 'petiot',\n",
       " 52069: 'most\\x85and',\n",
       " 3930: 'dinosaurs',\n",
       " 352: 'wrong',\n",
       " 52070: 'seda',\n",
       " 52071: 'stollen',\n",
       " 34712: 'sentencing',\n",
       " 40853: 'ouroboros',\n",
       " 40854: 'assimilates',\n",
       " 40855: 'colorfully',\n",
       " 27636: 'glenne',\n",
       " 52072: 'dongen',\n",
       " 4760: 'subplots',\n",
       " 52073: 'kiloton',\n",
       " 23381: 'chandon',\n",
       " 34713: \"effect'\",\n",
       " 27637: 'snugly',\n",
       " 40856: 'kuei',\n",
       " 9092: 'welcomed',\n",
       " 30071: 'dishonor',\n",
       " 52075: 'concurrence',\n",
       " 23382: 'stoicism',\n",
       " 14896: \"guys'\",\n",
       " 52077: \"beroemd'\",\n",
       " 6703: 'butcher',\n",
       " 40857: \"melfi's\",\n",
       " 30623: 'aargh',\n",
       " 20599: 'playhouse',\n",
       " 11308: 'wickedly',\n",
       " 1180: 'fit',\n",
       " 52078: 'labratory',\n",
       " 40859: 'lifeline',\n",
       " 1927: 'screaming',\n",
       " 4287: 'fix',\n",
       " 52079: 'cineliterate',\n",
       " 52080: 'fic',\n",
       " 52081: 'fia',\n",
       " 34714: 'fig',\n",
       " 52082: 'fmvs',\n",
       " 52083: 'fie',\n",
       " 52084: 'reentered',\n",
       " 30574: 'fin',\n",
       " 52085: 'doctresses',\n",
       " 52086: 'fil',\n",
       " 12606: 'zucker',\n",
       " 31931: 'ached',\n",
       " 52088: 'counsil',\n",
       " 52089: 'paterfamilias',\n",
       " 13885: 'songwriter',\n",
       " 34715: 'shivam',\n",
       " 9654: 'hurting',\n",
       " 299: 'effects',\n",
       " 52090: 'slauther',\n",
       " 52091: \"'flame'\",\n",
       " 52092: 'sommerset',\n",
       " 52093: 'interwhined',\n",
       " 27638: 'whacking',\n",
       " 52094: 'bartok',\n",
       " 8775: 'barton',\n",
       " 21909: 'frewer',\n",
       " 52095: \"fi'\",\n",
       " 6192: 'ingrid',\n",
       " 30575: 'stribor',\n",
       " 52096: 'approporiately',\n",
       " 52097: 'wobblyhand',\n",
       " 52098: 'tantalisingly',\n",
       " 52099: 'ankylosaurus',\n",
       " 17634: 'parasites',\n",
       " 52100: 'childen',\n",
       " 52101: \"jenkins'\",\n",
       " 52102: 'metafiction',\n",
       " 17635: 'golem',\n",
       " 40860: 'indiscretion',\n",
       " 23383: \"reeves'\",\n",
       " 57781: \"inamorata's\",\n",
       " 52104: 'brittannica',\n",
       " 7916: 'adapt',\n",
       " 30576: \"russo's\",\n",
       " 48246: 'guitarists',\n",
       " 10553: 'abbott',\n",
       " 40861: 'abbots',\n",
       " 17649: 'lanisha',\n",
       " 40863: 'magickal',\n",
       " 52105: 'mattter',\n",
       " 52106: \"'willy\",\n",
       " 34716: 'pumpkins',\n",
       " 52107: 'stuntpeople',\n",
       " 30577: 'estimate',\n",
       " 40864: 'ugghhh',\n",
       " 11309: 'gameplay',\n",
       " 52108: \"wern't\",\n",
       " 40865: \"n'sync\",\n",
       " 16117: 'sickeningly',\n",
       " 40866: 'chiara',\n",
       " 4011: 'disturbed',\n",
       " 40867: 'portmanteau',\n",
       " 52109: 'ineffectively',\n",
       " 82143: \"duchonvey's\",\n",
       " 37519: \"nasty'\",\n",
       " 1285: 'purpose',\n",
       " 52112: 'lazers',\n",
       " 28105: 'lightened',\n",
       " 52113: 'kaliganj',\n",
       " 52114: 'popularism',\n",
       " 18511: \"damme's\",\n",
       " 30578: 'stylistics',\n",
       " 52115: 'mindgaming',\n",
       " 46449: 'spoilerish',\n",
       " 52117: \"'corny'\",\n",
       " 34718: 'boerner',\n",
       " 6792: 'olds',\n",
       " 52118: 'bakelite',\n",
       " 27639: 'renovated',\n",
       " 27640: 'forrester',\n",
       " 52119: \"lumiere's\",\n",
       " 52024: 'gaskets',\n",
       " 884: 'needed',\n",
       " 34719: 'smight',\n",
       " 1297: 'master',\n",
       " 25905: \"edie's\",\n",
       " 40868: 'seeber',\n",
       " 52120: 'hiya',\n",
       " 52121: 'fuzziness',\n",
       " 14897: 'genesis',\n",
       " 12607: 'rewards',\n",
       " 30579: 'enthrall',\n",
       " 40869: \"'about\",\n",
       " 52122: \"recollection's\",\n",
       " 11039: 'mutilated',\n",
       " 52123: 'fatherlands',\n",
       " 52124: \"fischer's\",\n",
       " 5399: 'positively',\n",
       " 34705: '270',\n",
       " 34720: 'ahmed',\n",
       " 9836: 'zatoichi',\n",
       " 13886: 'bannister',\n",
       " 52127: 'anniversaries',\n",
       " 30580: \"helm's\",\n",
       " 52128: \"'work'\",\n",
       " 34721: 'exclaimed',\n",
       " 52129: \"'unfunny'\",\n",
       " 52029: '274',\n",
       " 544: 'feeling',\n",
       " 52131: \"wanda's\",\n",
       " 33266: 'dolan',\n",
       " 52133: '278',\n",
       " 52134: 'peacoat',\n",
       " 40870: 'brawny',\n",
       " 40871: 'mishra',\n",
       " 40872: 'worlders',\n",
       " 52135: 'protags',\n",
       " 52136: 'skullcap',\n",
       " 57596: 'dastagir',\n",
       " 5622: 'affairs',\n",
       " 7799: 'wholesome',\n",
       " 52137: 'hymen',\n",
       " 25246: 'paramedics',\n",
       " 52138: 'unpersons',\n",
       " 52139: 'heavyarms',\n",
       " 52140: 'affaire',\n",
       " 52141: 'coulisses',\n",
       " 40873: 'hymer',\n",
       " 52142: 'kremlin',\n",
       " 30581: 'shipments',\n",
       " 52143: 'pixilated',\n",
       " 30582: \"'00s\",\n",
       " 18512: 'diminishing',\n",
       " 1357: 'cinematic',\n",
       " 14898: 'resonates',\n",
       " 40874: 'simplify',\n",
       " 40875: \"nature'\",\n",
       " 40876: 'temptresses',\n",
       " 16822: 'reverence',\n",
       " 19502: 'resonated',\n",
       " 34722: 'dailey',\n",
       " 52144: '2\\x85',\n",
       " 27641: 'treize',\n",
       " 52145: 'majo',\n",
       " 21910: 'kiya',\n",
       " 52146: 'woolnough',\n",
       " 39797: 'thanatos',\n",
       " 35731: 'sandoval',\n",
       " 40879: 'dorama',\n",
       " 52147: \"o'shaughnessy\",\n",
       " 4988: 'tech',\n",
       " 32018: 'fugitives',\n",
       " 30583: 'teck',\n",
       " 76125: \"'e'\",\n",
       " 40881: 'doesn’t',\n",
       " 52149: 'purged',\n",
       " 657: 'saying',\n",
       " 41095: \"martians'\",\n",
       " 23418: 'norliss',\n",
       " 27642: 'dickey',\n",
       " 52152: 'dicker',\n",
       " 52153: \"'sependipity\",\n",
       " 8422: 'padded',\n",
       " 57792: 'ordell',\n",
       " 40882: \"sturges'\",\n",
       " 52154: 'independentcritics',\n",
       " 5745: 'tempted',\n",
       " 34724: \"atkinson's\",\n",
       " 25247: 'hounded',\n",
       " 52155: 'apace',\n",
       " 15494: 'clicked',\n",
       " 30584: \"'humor'\",\n",
       " 17177: \"martino's\",\n",
       " 52156: \"'supporting\",\n",
       " 52032: 'warmongering',\n",
       " 34725: \"zemeckis's\",\n",
       " 21911: 'lube',\n",
       " 52157: 'shocky',\n",
       " 7476: 'plate',\n",
       " 40883: 'plata',\n",
       " 40884: 'sturgess',\n",
       " 40885: \"nerds'\",\n",
       " 20600: 'plato',\n",
       " 34726: 'plath',\n",
       " 40886: 'platt',\n",
       " 52159: 'mcnab',\n",
       " 27643: 'clumsiness',\n",
       " 3899: 'altogether',\n",
       " 42584: 'massacring',\n",
       " 52160: 'bicenntinial',\n",
       " 40887: 'skaal',\n",
       " 14360: 'droning',\n",
       " 8776: 'lds',\n",
       " 21912: 'jaguar',\n",
       " 34727: \"cale's\",\n",
       " 1777: 'nicely',\n",
       " 4588: 'mummy',\n",
       " 18513: \"lot's\",\n",
       " 10086: 'patch',\n",
       " 50202: 'kerkhof',\n",
       " 52161: \"leader's\",\n",
       " 27644: \"'movie\",\n",
       " 52162: 'uncomfirmed',\n",
       " 40888: 'heirloom',\n",
       " 47360: 'wrangle',\n",
       " 52163: 'emotion\\x85',\n",
       " 52164: \"'stargate'\",\n",
       " 40889: 'pinoy',\n",
       " 40890: 'conchatta',\n",
       " 41128: 'broeke',\n",
       " 40891: 'advisedly',\n",
       " 17636: \"barker's\",\n",
       " 52166: 'descours',\n",
       " 772: 'lots',\n",
       " 9259: 'lotr',\n",
       " 9879: 'irs',\n",
       " 52167: 'lott',\n",
       " 40892: 'xvi',\n",
       " 34728: 'irk',\n",
       " 52168: 'irl',\n",
       " 6887: 'ira',\n",
       " 21913: 'belzer',\n",
       " 52169: 'irc',\n",
       " 27645: 'ire',\n",
       " 40893: 'requisites',\n",
       " 7693: 'discipline',\n",
       " 52961: 'lyoko',\n",
       " 11310: 'extend',\n",
       " 873: 'nature',\n",
       " 52170: \"'dickie'\",\n",
       " 40894: 'optimist',\n",
       " 30586: 'lapping',\n",
       " 3900: 'superficial',\n",
       " 52171: 'vestment',\n",
       " 2823: 'extent',\n",
       " 52172: 'tendons',\n",
       " 52173: \"heller's\",\n",
       " 52174: 'quagmires',\n",
       " 52175: 'miyako',\n",
       " 20601: 'moocow',\n",
       " 52176: \"coles'\",\n",
       " 40895: 'lookit',\n",
       " 52177: 'ravenously',\n",
       " 40896: 'levitating',\n",
       " 52178: 'perfunctorily',\n",
       " 30587: 'lookin',\n",
       " 40898: \"lot'\",\n",
       " 52179: 'lookie',\n",
       " 34870: 'fearlessly',\n",
       " 52181: 'libyan',\n",
       " 40899: 'fondles',\n",
       " 35714: 'gopher',\n",
       " 40901: 'wearying',\n",
       " 52182: \"nz's\",\n",
       " 27646: 'minuses',\n",
       " 52183: 'puposelessly',\n",
       " 52184: 'shandling',\n",
       " 31268: 'decapitates',\n",
       " 11929: 'humming',\n",
       " 40902: \"'nother\",\n",
       " 21914: 'smackdown',\n",
       " 30588: 'underdone',\n",
       " 40903: 'frf',\n",
       " 52185: 'triviality',\n",
       " 25248: 'fro',\n",
       " 8777: 'bothers',\n",
       " 52186: \"'kensington\",\n",
       " 73: 'much',\n",
       " 34730: 'muco',\n",
       " 22615: 'wiseguy',\n",
       " 27648: \"richie's\",\n",
       " 40904: 'tonino',\n",
       " 52187: 'unleavened',\n",
       " 11587: 'fry',\n",
       " 40905: \"'tv'\",\n",
       " 40906: 'toning',\n",
       " 14361: 'obese',\n",
       " 30589: 'sensationalized',\n",
       " 40907: 'spiv',\n",
       " 6259: 'spit',\n",
       " 7364: 'arkin',\n",
       " 21915: 'charleton',\n",
       " 16823: 'jeon',\n",
       " 21916: 'boardroom',\n",
       " 4989: 'doubts',\n",
       " 3084: 'spin',\n",
       " 53083: 'hepo',\n",
       " 27649: 'wildcat',\n",
       " 10584: 'venoms',\n",
       " 52191: 'misconstrues',\n",
       " 18514: 'mesmerising',\n",
       " 40908: 'misconstrued',\n",
       " 52192: 'rescinds',\n",
       " 52193: 'prostrate',\n",
       " 40909: 'majid',\n",
       " 16479: 'climbed',\n",
       " 34731: 'canoeing',\n",
       " 52195: 'majin',\n",
       " 57804: 'animie',\n",
       " 40910: 'sylke',\n",
       " 14899: 'conditioned',\n",
       " 40911: 'waddell',\n",
       " 52196: '3\\x85',\n",
       " 41188: 'hyperdrive',\n",
       " 34732: 'conditioner',\n",
       " 53153: 'bricklayer',\n",
       " 2576: 'hong',\n",
       " 52198: 'memoriam',\n",
       " 30592: 'inventively',\n",
       " 25249: \"levant's\",\n",
       " 20638: 'portobello',\n",
       " 52200: 'remand',\n",
       " 19504: 'mummified',\n",
       " 27650: 'honk',\n",
       " 19505: 'spews',\n",
       " 40912: 'visitations',\n",
       " 52201: 'mummifies',\n",
       " 25250: 'cavanaugh',\n",
       " 23385: 'zeon',\n",
       " 40913: \"jungle's\",\n",
       " 34733: 'viertel',\n",
       " 27651: 'frenchmen',\n",
       " 52202: 'torpedoes',\n",
       " 52203: 'schlessinger',\n",
       " 34734: 'torpedoed',\n",
       " 69876: 'blister',\n",
       " 52204: 'cinefest',\n",
       " 34735: 'furlough',\n",
       " 52205: 'mainsequence',\n",
       " 40914: 'mentors',\n",
       " 9094: 'academic',\n",
       " 20602: 'stillness',\n",
       " 40915: 'academia',\n",
       " 52206: 'lonelier',\n",
       " 52207: 'nibby',\n",
       " 52208: \"losers'\",\n",
       " 40916: 'cineastes',\n",
       " 4449: 'corporate',\n",
       " 40917: 'massaging',\n",
       " 30593: 'bellow',\n",
       " 19506: 'absurdities',\n",
       " 53241: 'expetations',\n",
       " 40918: 'nyfiken',\n",
       " 75638: 'mehras',\n",
       " 52209: 'lasse',\n",
       " 52210: 'visability',\n",
       " 33946: 'militarily',\n",
       " 52211: \"elder'\",\n",
       " 19023: 'gainsbourg',\n",
       " 20603: 'hah',\n",
       " 13420: 'hai',\n",
       " 34736: 'haj',\n",
       " 25251: 'hak',\n",
       " 4311: 'hal',\n",
       " 4892: 'ham',\n",
       " 53259: 'duffer',\n",
       " 52213: 'haa',\n",
       " 66: 'had',\n",
       " 11930: 'advancement',\n",
       " 16825: 'hag',\n",
       " 25252: \"hand'\",\n",
       " 13421: 'hay',\n",
       " 20604: 'mcnamara',\n",
       " 52214: \"mozart's\",\n",
       " 30731: 'duffel',\n",
       " 30594: 'haq',\n",
       " 13887: 'har',\n",
       " 44: 'has',\n",
       " 2401: 'hat',\n",
       " 40919: 'hav',\n",
       " 30595: 'haw',\n",
       " 52215: 'figtings',\n",
       " 15495: 'elders',\n",
       " 52216: 'underpanted',\n",
       " 52217: 'pninson',\n",
       " 27652: 'unequivocally',\n",
       " 23673: \"barbara's\",\n",
       " 52219: \"bello'\",\n",
       " 12997: 'indicative',\n",
       " 40920: 'yawnfest',\n",
       " 52220: 'hexploitation',\n",
       " 52221: \"loder's\",\n",
       " 27653: 'sleuthing',\n",
       " 32622: \"justin's\",\n",
       " 52222: \"'ball\",\n",
       " 52223: \"'summer\",\n",
       " 34935: \"'demons'\",\n",
       " 52225: \"mormon's\",\n",
       " 34737: \"laughton's\",\n",
       " 52226: 'debell',\n",
       " 39724: 'shipyard',\n",
       " 30597: 'unabashedly',\n",
       " 40401: 'disks',\n",
       " 2290: 'crowd',\n",
       " 10087: 'crowe',\n",
       " 56434: \"vancouver's\",\n",
       " 34738: 'mosques',\n",
       " 6627: 'crown',\n",
       " 52227: 'culpas',\n",
       " 27654: 'crows',\n",
       " 53344: 'surrell',\n",
       " 52229: 'flowless',\n",
       " 52230: 'sheirk',\n",
       " 40923: \"'three\",\n",
       " 52231: \"peterson'\",\n",
       " 52232: 'ooverall',\n",
       " 40924: 'perchance',\n",
       " 1321: 'bottom',\n",
       " 53363: 'chabert',\n",
       " 52233: 'sneha',\n",
       " 13888: 'inhuman',\n",
       " 52234: 'ichii',\n",
       " 52235: 'ursla',\n",
       " 30598: 'completly',\n",
       " 40925: 'moviedom',\n",
       " 52236: 'raddick',\n",
       " 51995: 'brundage',\n",
       " 40926: 'brigades',\n",
       " 1181: 'starring',\n",
       " 52237: \"'goal'\",\n",
       " 52238: 'caskets',\n",
       " 52239: 'willcock',\n",
       " 52240: \"threesome's\",\n",
       " 52241: \"mosque'\",\n",
       " 52242: \"cover's\",\n",
       " 17637: 'spaceships',\n",
       " 40927: 'anomalous',\n",
       " 27655: 'ptsd',\n",
       " 52243: 'shirdan',\n",
       " 21962: 'obscenity',\n",
       " 30599: 'lemmings',\n",
       " 30600: 'duccio',\n",
       " 52244: \"levene's\",\n",
       " 52245: \"'gorby'\",\n",
       " 25255: \"teenager's\",\n",
       " 5340: 'marshall',\n",
       " 9095: 'honeymoon',\n",
       " 3231: 'shoots',\n",
       " 12258: 'despised',\n",
       " 52246: 'okabasho',\n",
       " 8289: 'fabric',\n",
       " 18515: 'cannavale',\n",
       " 3537: 'raped',\n",
       " 52247: \"tutt's\",\n",
       " 17638: 'grasping',\n",
       " 18516: 'despises',\n",
       " 40928: \"thief's\",\n",
       " 8926: 'rapes',\n",
       " 52248: 'raper',\n",
       " 27656: \"eyre'\",\n",
       " 52249: 'walchek',\n",
       " 23386: \"elmo's\",\n",
       " 40929: 'perfumes',\n",
       " 21918: 'spurting',\n",
       " 52250: \"exposition'\\x85\",\n",
       " 52251: 'denoting',\n",
       " 34740: 'thesaurus',\n",
       " 40930: \"shoot'\",\n",
       " 49759: 'bonejack',\n",
       " 52253: 'simpsonian',\n",
       " 30601: 'hebetude',\n",
       " 34741: \"hallow's\",\n",
       " 52254: 'desperation\\x85',\n",
       " 34742: 'incinerator',\n",
       " 10308: 'congratulations',\n",
       " 52255: 'humbled',\n",
       " 5924: \"else's\",\n",
       " 40845: 'trelkovski',\n",
       " 52256: \"rape'\",\n",
       " 59386: \"'chapters'\",\n",
       " 52257: '1600s',\n",
       " 7253: 'martian',\n",
       " 25256: 'nicest',\n",
       " 52259: 'eyred',\n",
       " 9457: 'passenger',\n",
       " 6041: 'disgrace',\n",
       " 52260: 'moderne',\n",
       " 5120: 'barrymore',\n",
       " 52261: 'yankovich',\n",
       " 40931: 'moderns',\n",
       " 52262: 'studliest',\n",
       " 52263: 'bedsheet',\n",
       " 14900: 'decapitation',\n",
       " 52264: 'slurring',\n",
       " 52265: \"'nunsploitation'\",\n",
       " 34743: \"'character'\",\n",
       " 9880: 'cambodia',\n",
       " 52266: 'rebelious',\n",
       " 27657: 'pasadena',\n",
       " 40932: 'crowne',\n",
       " 52267: \"'bedchamber\",\n",
       " 52268: 'conjectural',\n",
       " 52269: 'appologize',\n",
       " 52270: 'halfassing',\n",
       " 57816: 'paycheque',\n",
       " 20606: 'palms',\n",
       " 52271: \"'islands\",\n",
       " 40933: 'hawked',\n",
       " 21919: 'palme',\n",
       " 40934: 'conservatively',\n",
       " 64007: 'larp',\n",
       " 5558: 'palma',\n",
       " 21920: 'smelling',\n",
       " 12998: 'aragorn',\n",
       " 52272: 'hawker',\n",
       " 52273: 'hawkes',\n",
       " 3975: 'explosions',\n",
       " 8059: 'loren',\n",
       " 52274: \"pyle's\",\n",
       " 6704: 'shootout',\n",
       " 18517: \"mike's\",\n",
       " 52275: \"driscoll's\",\n",
       " 40935: 'cogsworth',\n",
       " 52276: \"britian's\",\n",
       " 34744: 'childs',\n",
       " 52277: \"portrait's\",\n",
       " 3626: 'chain',\n",
       " 2497: 'whoever',\n",
       " 52278: 'puttered',\n",
       " 52279: 'childe',\n",
       " 52280: 'maywether',\n",
       " 3036: 'chair',\n",
       " 52281: \"rance's\",\n",
       " 34745: 'machu',\n",
       " 4517: 'ballet',\n",
       " 34746: 'grapples',\n",
       " 76152: 'summerize',\n",
       " 30603: 'freelance',\n",
       " 52283: \"andrea's\",\n",
       " 52284: '\\x91very',\n",
       " 45879: 'coolidge',\n",
       " 18518: 'mache',\n",
       " 52285: 'balled',\n",
       " 40937: 'grappled',\n",
       " 18519: 'macha',\n",
       " 21921: 'underlining',\n",
       " 5623: 'macho',\n",
       " 19507: 'oversight',\n",
       " 25257: 'machi',\n",
       " 11311: 'verbally',\n",
       " 21922: 'tenacious',\n",
       " 40938: 'windshields',\n",
       " 18557: 'paychecks',\n",
       " 3396: 'jerk',\n",
       " 11931: \"good'\",\n",
       " 34748: 'prancer',\n",
       " 21923: 'prances',\n",
       " 52286: 'olympus',\n",
       " 21924: 'lark',\n",
       " 10785: 'embark',\n",
       " 7365: 'gloomy',\n",
       " 52287: 'jehaan',\n",
       " 52288: 'turaqui',\n",
       " 20607: \"child'\",\n",
       " 2894: 'locked',\n",
       " 52289: 'pranced',\n",
       " 2588: 'exact',\n",
       " 52290: 'unattuned',\n",
       " 783: 'minute',\n",
       " 16118: 'skewed',\n",
       " 40940: 'hodgins',\n",
       " 34749: 'skewer',\n",
       " 52291: 'think\\x85',\n",
       " 38765: 'rosenstein',\n",
       " 52292: 'helmit',\n",
       " 34750: 'wrestlemanias',\n",
       " 16826: 'hindered',\n",
       " 30604: \"martha's\",\n",
       " 52293: 'cheree',\n",
       " 52294: \"pluckin'\",\n",
       " 40941: 'ogles',\n",
       " 11932: 'heavyweight',\n",
       " 82190: 'aada',\n",
       " 11312: 'chopping',\n",
       " 61534: 'strongboy',\n",
       " 41342: 'hegemonic',\n",
       " 40942: 'adorns',\n",
       " 41346: 'xxth',\n",
       " 34751: 'nobuhiro',\n",
       " 52298: 'capitães',\n",
       " 52299: 'kavogianni',\n",
       " 13422: 'antwerp',\n",
       " 6538: 'celebrated',\n",
       " 52300: 'roarke',\n",
       " 40943: 'baggins',\n",
       " 31270: 'cheeseburgers',\n",
       " 52301: 'matras',\n",
       " 52302: \"nineties'\",\n",
       " 52303: \"'craig'\",\n",
       " 12999: 'celebrates',\n",
       " 3383: 'unintentionally',\n",
       " 14362: 'drafted',\n",
       " 52304: 'climby',\n",
       " 52305: '303',\n",
       " 18520: 'oldies',\n",
       " 9096: 'climbs',\n",
       " 9655: 'honour',\n",
       " 34752: 'plucking',\n",
       " 30074: '305',\n",
       " 5514: 'address',\n",
       " 40944: 'menjou',\n",
       " 42592: \"'freak'\",\n",
       " 19508: 'dwindling',\n",
       " 9458: 'benson',\n",
       " 52307: 'white’s',\n",
       " 40945: 'shamelessness',\n",
       " 21925: 'impacted',\n",
       " 52308: 'upatz',\n",
       " 3840: 'cusack',\n",
       " 37567: \"flavia's\",\n",
       " 52309: 'effette',\n",
       " 34753: 'influx',\n",
       " 52310: 'boooooooo',\n",
       " 52311: 'dimitrova',\n",
       " 13423: 'houseman',\n",
       " 25259: 'bigas',\n",
       " 52312: 'boylen',\n",
       " 52313: 'phillipenes',\n",
       " 40946: 'fakery',\n",
       " 27658: \"grandpa's\",\n",
       " 27659: 'darnell',\n",
       " 19509: 'undergone',\n",
       " 52315: 'handbags',\n",
       " 21926: 'perished',\n",
       " 37778: 'pooped',\n",
       " 27660: 'vigour',\n",
       " 3627: 'opposed',\n",
       " 52316: 'etude',\n",
       " 11799: \"caine's\",\n",
       " 52317: 'doozers',\n",
       " 34754: 'photojournals',\n",
       " 52318: 'perishes',\n",
       " 34755: 'constrains',\n",
       " 40948: 'migenes',\n",
       " 30605: 'consoled',\n",
       " 16827: 'alastair',\n",
       " 52319: 'wvs',\n",
       " 52320: 'ooooooh',\n",
       " 34756: 'approving',\n",
       " 40949: 'consoles',\n",
       " 52064: 'disparagement',\n",
       " 52322: 'futureistic',\n",
       " 52323: 'rebounding',\n",
       " 52324: \"'date\",\n",
       " 52325: 'gregoire',\n",
       " 21927: 'rutherford',\n",
       " 34757: 'americanised',\n",
       " 82196: 'novikov',\n",
       " 1042: 'following',\n",
       " 34758: 'munroe',\n",
       " 52326: \"morita'\",\n",
       " 52327: 'christenssen',\n",
       " 23106: 'oatmeal',\n",
       " 25260: 'fossey',\n",
       " 40950: 'livered',\n",
       " 13000: 'listens',\n",
       " 76164: \"'marci\",\n",
       " 52330: \"otis's\",\n",
       " 23387: 'thanking',\n",
       " 16019: 'maude',\n",
       " 34759: 'extensions',\n",
       " 52332: 'ameteurish',\n",
       " 52333: \"commender's\",\n",
       " 27661: 'agricultural',\n",
       " 4518: 'convincingly',\n",
       " 17639: 'fueled',\n",
       " 54014: 'mahattan',\n",
       " 40952: \"paris's\",\n",
       " 52336: 'vulkan',\n",
       " 52337: 'stapes',\n",
       " 52338: 'odysessy',\n",
       " 12259: 'harmon',\n",
       " 4252: 'surfing',\n",
       " 23494: 'halloran',\n",
       " 49580: 'unbelieveably',\n",
       " 52339: \"'offed'\",\n",
       " 30607: 'quadrant',\n",
       " 19510: 'inhabiting',\n",
       " 34760: 'nebbish',\n",
       " 40953: 'forebears',\n",
       " 34761: 'skirmish',\n",
       " 52340: 'ocassionally',\n",
       " 52341: \"'resist\",\n",
       " 21928: 'impactful',\n",
       " 52342: 'spicier',\n",
       " 40954: 'touristy',\n",
       " 52343: \"'football'\",\n",
       " 40955: 'webpage',\n",
       " 52345: 'exurbia',\n",
       " 52346: 'jucier',\n",
       " 14901: 'professors',\n",
       " 34762: 'structuring',\n",
       " 30608: 'jig',\n",
       " 40956: 'overlord',\n",
       " 25261: 'disconnect',\n",
       " 82201: 'sniffle',\n",
       " 40957: 'slimeball',\n",
       " 40958: 'jia',\n",
       " 16828: 'milked',\n",
       " 40959: 'banjoes',\n",
       " 1237: 'jim',\n",
       " 52348: 'workforces',\n",
       " 52349: 'jip',\n",
       " 52350: 'rotweiller',\n",
       " 34763: 'mundaneness',\n",
       " 52351: \"'ninja'\",\n",
       " 11040: \"dead'\",\n",
       " 40960: \"cipriani's\",\n",
       " 20608: 'modestly',\n",
       " 52352: \"professor'\",\n",
       " 40961: 'shacked',\n",
       " 34764: 'bashful',\n",
       " 23388: 'sorter',\n",
       " 16120: 'overpowering',\n",
       " 18521: 'workmanlike',\n",
       " 27662: 'henpecked',\n",
       " 18522: 'sorted',\n",
       " 52354: \"jōb's\",\n",
       " 52355: \"'always\",\n",
       " 34765: \"'baptists\",\n",
       " 52356: 'dreamcatchers',\n",
       " 52357: \"'silence'\",\n",
       " 21929: 'hickory',\n",
       " 52358: 'fun\\x97yet',\n",
       " 52359: 'breakumentary',\n",
       " 15496: 'didn',\n",
       " 52360: 'didi',\n",
       " 52361: 'pealing',\n",
       " 40962: 'dispite',\n",
       " 25262: \"italy's\",\n",
       " 21930: 'instability',\n",
       " 6539: 'quarter',\n",
       " 12608: 'quartet',\n",
       " 52362: 'padmé',\n",
       " 52363: \"'bleedmedry\",\n",
       " 52364: 'pahalniuk',\n",
       " 52365: 'honduras',\n",
       " 10786: 'bursting',\n",
       " 41465: \"pablo's\",\n",
       " 52367: 'irremediably',\n",
       " 40963: 'presages',\n",
       " 57832: 'bowlegged',\n",
       " 65183: 'dalip',\n",
       " 6260: 'entering',\n",
       " 76172: 'newsradio',\n",
       " 54150: 'presaged',\n",
       " 27663: \"giallo's\",\n",
       " 40964: 'bouyant',\n",
       " 52368: 'amerterish',\n",
       " 18523: 'rajni',\n",
       " 30610: 'leeves',\n",
       " 34767: 'macauley',\n",
       " 612: 'seriously',\n",
       " 52369: 'sugercoma',\n",
       " 52370: 'grimstead',\n",
       " 52371: \"'fairy'\",\n",
       " 30611: 'zenda',\n",
       " 52372: \"'twins'\",\n",
       " 17640: 'realisation',\n",
       " 27664: 'highsmith',\n",
       " 7817: 'raunchy',\n",
       " 40965: 'incentives',\n",
       " 52374: 'flatson',\n",
       " 35097: 'snooker',\n",
       " 16829: 'crazies',\n",
       " 14902: 'crazier',\n",
       " 7094: 'grandma',\n",
       " 52375: 'napunsaktha',\n",
       " 30612: 'workmanship',\n",
       " 52376: 'reisner',\n",
       " 61306: \"sanford's\",\n",
       " 52377: '\\x91doña',\n",
       " 6108: 'modest',\n",
       " 19153: \"everything's\",\n",
       " 40966: 'hamer',\n",
       " 52379: \"couldn't'\",\n",
       " 13001: 'quibble',\n",
       " 52380: 'socking',\n",
       " 21931: 'tingler',\n",
       " 52381: 'gutman',\n",
       " 40967: 'lachlan',\n",
       " 52382: 'tableaus',\n",
       " 52383: 'headbanger',\n",
       " 2847: 'spoken',\n",
       " 34768: 'cerebrally',\n",
       " 23490: \"'road\",\n",
       " 21932: 'tableaux',\n",
       " 40968: \"proust's\",\n",
       " 40969: 'periodical',\n",
       " 52385: \"shoveller's\",\n",
       " 25263: 'tamara',\n",
       " 17641: 'affords',\n",
       " 3249: 'concert',\n",
       " 87955: \"yara's\",\n",
       " 52386: 'someome',\n",
       " 8424: 'lingering',\n",
       " 41511: \"abraham's\",\n",
       " 34769: 'beesley',\n",
       " 34770: 'cherbourg',\n",
       " 28624: 'kagan',\n",
       " 9097: 'snatch',\n",
       " 9260: \"miyazaki's\",\n",
       " 25264: 'absorbs',\n",
       " 40970: \"koltai's\",\n",
       " 64027: 'tingled',\n",
       " 19511: 'crossroads',\n",
       " 16121: 'rehab',\n",
       " 52389: 'falworth',\n",
       " 52390: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "passing-complement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "domestic-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cloudy-disco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-tournament",
   "metadata": {},
   "source": [
    "> 여기서 하는 전처리는 Tensorflow 튜토리얼에서의 전처리를 따왔단다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bacterial-impact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "focal-thumb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "injured-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "# encode된 게 제대로 우리 decode 함수에 작동하는지 확인\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-berkeley",
   "metadata": {},
   "source": [
    "> padding 추가가 필요함  \n",
    "> 근데 padding의 max_len 값도 사실 모델의 성능, 속도에 많은 영향을 미친다.  \n",
    "> 적절한 값을 찾기 위해선 전체 데이터셋의 분포를 확인하는 게 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ideal-refrigerator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-nudist",
   "metadata": {},
   "source": [
    "> 여기서는 정규분포의 특성을 이용해서 평균 + 2*std인 곳이 전체 데이터의 약 95%를 차지한다는 가설을 갖고 max_len을 설정해보고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-island",
   "metadata": {},
   "source": [
    "> padding을 pre 혹은 post로 하느냐에 따라 성능이 달라지는데 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pretty-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "# post로 패딩한 데이터\n",
    "x_train_post = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test_post = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "approximate-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "# pre로 패딩한 데이터\n",
    "x_train_pre = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test_pre = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train_pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-institute",
   "metadata": {},
   "source": [
    "## # 모델 직접 설계 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "orange-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,401\n",
      "Trainable params: 162,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model_lstm = tf.keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(16))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model_lstm.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "complimentary-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "# post 데이터 사용\n",
    "x_val_post = x_train_post[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train_post = x_train_post[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train_post.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "intelligent-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 7s 137ms/step - loss: 0.6931 - accuracy: 0.5048 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.6930 - accuracy: 0.5121 - val_loss: 0.6930 - val_accuracy: 0.5017\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 3s 115ms/step - loss: 0.6924 - accuracy: 0.5101 - val_loss: 0.6929 - val_accuracy: 0.5024\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6896 - accuracy: 0.5234 - val_loss: 0.6887 - val_accuracy: 0.5070\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6833 - accuracy: 0.5241 - val_loss: 0.6944 - val_accuracy: 0.4982\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6923 - accuracy: 0.5082 - val_loss: 0.6929 - val_accuracy: 0.5037\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6888 - accuracy: 0.5272 - val_loss: 0.6914 - val_accuracy: 0.5082\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.6851 - accuracy: 0.5303 - val_loss: 0.6884 - val_accuracy: 0.5094\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.6775 - accuracy: 0.5289 - val_loss: 0.6870 - val_accuracy: 0.5114\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.6743 - accuracy: 0.5306 - val_loss: 0.6861 - val_accuracy: 0.5127\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.6711 - accuracy: 0.5333 - val_loss: 0.6858 - val_accuracy: 0.5149\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.6684 - accuracy: 0.5411 - val_loss: 0.6883 - val_accuracy: 0.5177\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.6635 - accuracy: 0.5449 - val_loss: 0.6684 - val_accuracy: 0.6924\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.6585 - accuracy: 0.6191 - val_loss: 0.6381 - val_accuracy: 0.6499\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.6088 - accuracy: 0.6850 - val_loss: 0.5994 - val_accuracy: 0.7087\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.5791 - accuracy: 0.7338 - val_loss: 0.5883 - val_accuracy: 0.7244\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.5851 - accuracy: 0.7255 - val_loss: 0.5882 - val_accuracy: 0.7246\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.5754 - accuracy: 0.7348 - val_loss: 0.5878 - val_accuracy: 0.7245\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.5815 - accuracy: 0.7281 - val_loss: 0.5881 - val_accuracy: 0.7245\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.5698 - accuracy: 0.7391 - val_loss: 0.5861 - val_accuracy: 0.7251\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# post train, post val 사용\n",
    "model_lstm.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model_lstm.fit(partial_x_train_post,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val_post, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "crude-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 23s - loss: 0.5926 - accuracy: 0.7184\n",
      "[0.5926341414451599, 0.7183600068092346]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋으로 평가하기\n",
    "results = model_lstm.evaluate(x_test_post,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "completed-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# 그래프 그릴 항목 확인\n",
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "athletic-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-korea",
   "metadata": {},
   "source": [
    "> validation loss와 train loss간에 이격이 발생하는 지점이 있다면 그 이상의 트레이닝은 무의미해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greek-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-chancellor",
   "metadata": {},
   "source": [
    "> 마찬가지로 Training and validation accuracy를 그려 보아도 유사한 인사이트를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "deadly-device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 6s 134ms/step - loss: 0.6921 - accuracy: 0.5105 - val_loss: 0.6831 - val_accuracy: 0.5868\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.6680 - accuracy: 0.6231 - val_loss: 0.6538 - val_accuracy: 0.6656\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.6059 - accuracy: 0.7441 - val_loss: 0.5455 - val_accuracy: 0.8133\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.4876 - accuracy: 0.8360 - val_loss: 0.4249 - val_accuracy: 0.8412\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.3515 - accuracy: 0.8826 - val_loss: 0.3320 - val_accuracy: 0.8684\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.2389 - accuracy: 0.9209 - val_loss: 0.3178 - val_accuracy: 0.8712\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.1859 - accuracy: 0.9404 - val_loss: 0.3193 - val_accuracy: 0.8747\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 3s 103ms/step - loss: 0.1394 - accuracy: 0.9601 - val_loss: 0.3189 - val_accuracy: 0.8751\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 103ms/step - loss: 0.1092 - accuracy: 0.9707 - val_loss: 0.3734 - val_accuracy: 0.8720\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.0974 - accuracy: 0.9735 - val_loss: 0.3702 - val_accuracy: 0.8735\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 3s 100ms/step - loss: 0.0802 - accuracy: 0.9807 - val_loss: 0.3918 - val_accuracy: 0.8699\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 3s 99ms/step - loss: 0.0677 - accuracy: 0.9851 - val_loss: 0.4227 - val_accuracy: 0.8690\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.0492 - accuracy: 0.9900 - val_loss: 0.4335 - val_accuracy: 0.8651\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.0483 - accuracy: 0.9902 - val_loss: 0.4642 - val_accuracy: 0.8621\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.0543 - accuracy: 0.9874 - val_loss: 0.4699 - val_accuracy: 0.8635\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 3s 99ms/step - loss: 0.0352 - accuracy: 0.9932 - val_loss: 0.4586 - val_accuracy: 0.8644\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 98ms/step - loss: 0.0456 - accuracy: 0.9910 - val_loss: 0.5209 - val_accuracy: 0.8643\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 99ms/step - loss: 0.0364 - accuracy: 0.9930 - val_loss: 0.5289 - val_accuracy: 0.8565\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 3s 98ms/step - loss: 0.0338 - accuracy: 0.9939 - val_loss: 0.5295 - val_accuracy: 0.8601\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 3s 97ms/step - loss: 0.0256 - accuracy: 0.9960 - val_loss: 0.5655 - val_accuracy: 0.8619\n"
     ]
    }
   ],
   "source": [
    "# padding 'pre'로 했을 때의 모델 accuracy 확인\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model_lstm = tf.keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(16))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model_lstm.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "# validation set 10000건 분리\n",
    "# post 데이터 사용\n",
    "x_val_pre = x_train_pre[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train_pre = x_train_pre[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train_pre.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "# 모델 학습\n",
    "# post train, post val 사용\n",
    "model_lstm.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model_lstm.fit(partial_x_train_pre,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val_pre, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "little-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "vocal-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-medline",
   "metadata": {},
   "source": [
    "> `pre로 padding한 것은 압도적으로 예측이 좋게 나온다.  \n",
    "> 이 경우 EPOCH를 작게 하는 것이 좋겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-customs",
   "metadata": {},
   "source": [
    "## # Word2Vec 적용\n",
    "> 이미 Embedding 레이어를 통해 적용했지만 구체적으로 알아보기  \n",
    "> 워드 공간에 유의미하게 형성되어 있는지 파악해보기  \n",
    "> 해당 word vector를 다룰 때는 `gensim` 패키지 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "mighty-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model_lstm.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "vocal-rescue",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f3b24f26450>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f3b22607290>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3b24d1ff90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3b24c9f790>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(model_lstm.layers))\n",
    "model_lstm.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "printable-pound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10000, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00769743, -0.02441218,  0.0074533 , ...,  0.02452769,\n",
       "        -0.07689153, -0.05259864],\n",
       "       [ 0.10114594, -0.10399839,  0.10293962, ..., -0.04611583,\n",
       "        -0.20221464, -0.05011757],\n",
       "       [-0.02898518, -0.02474586,  0.01796851, ...,  0.05647976,\n",
       "         0.00189297, -0.00809398],\n",
       "       ...,\n",
       "       [ 0.05925551, -0.05830109,  0.03919606, ..., -0.02791787,\n",
       "        -0.04822203, -0.05354132],\n",
       "       [ 0.04130551,  0.01399222,  0.04475417, ...,  0.0182888 ,\n",
       "        -0.08064327, -0.0041659 ],\n",
       "       [ 0.02437731, -0.01732   ,  0.03807655, ..., -0.07236972,\n",
       "        -0.06833529, -0.07756536]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(weights))\n",
    "print(weights.shape)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-george",
   "metadata": {},
   "source": [
    "> 단어갯수만큼 행이 있고, out_dim으로 설정한 만큼의 열로 옆으로 늘어나 있는 행렬이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "hindu-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00769743, -0.02441218,  0.0074533 , ...,  0.02452769,\n",
       "         -0.07689153, -0.05259864],\n",
       "        [ 0.10114594, -0.10399839,  0.10293962, ..., -0.04611583,\n",
       "         -0.20221464, -0.05011757],\n",
       "        [-0.02898518, -0.02474586,  0.01796851, ...,  0.05647976,\n",
       "          0.00189297, -0.00809398],\n",
       "        ...,\n",
       "        [ 0.05925551, -0.05830109,  0.03919606, ..., -0.02791787,\n",
       "         -0.04822203, -0.05354132],\n",
       "        [ 0.04130551,  0.01399222,  0.04475417, ...,  0.0182888 ,\n",
       "         -0.08064327, -0.0041659 ],\n",
       "        [ 0.02437731, -0.01732   ,  0.03807655, ..., -0.07236972,\n",
       "         -0.06833529, -0.07756536]], dtype=float32)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adapted-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model_lstm.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "collective-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00974652, -0.02607295, -0.02527144, -0.03768079, -0.00119043,\n",
       "        0.04774732, -0.01291266,  0.05074356,  0.01908138,  0.0233574 ,\n",
       "       -0.05896892,  0.02538653, -0.03822448,  0.02092041,  0.03420791,\n",
       "        0.00914042], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-celebration",
   "metadata": {},
   "source": [
    "> `gensim`에서 제공하는 패키지를 이용해 위에 남긴 임베딩 파라미터를 읽어서 word vector로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "noted-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.02853958,  0.04240824,  0.03001517,  0.01901824,  0.03709544,\n",
       "       -0.02492859,  0.00222227, -0.02635476, -0.001752  , -0.01180111,\n",
       "        0.0696243 , -0.0443258 ,  0.01111723, -0.00723196, -0.05406441,\n",
       "       -0.03796557], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "expired-actress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " '_upconvert_old_d2vkv',\n",
       " '_upconvert_old_vocab',\n",
       " 'add_lifecycle_event',\n",
       " 'add_vector',\n",
       " 'add_vectors',\n",
       " 'allocate_vecattrs',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'expandos',\n",
       " 'fill_norms',\n",
       " 'get_index',\n",
       " 'get_normed_vectors',\n",
       " 'get_vecattr',\n",
       " 'get_vector',\n",
       " 'has_index_for',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'index_to_key',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'key_to_index',\n",
       " 'lifecycle_events',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'mapfile_path',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'next_index',\n",
       " 'norms',\n",
       " 'rank',\n",
       " 'rank_by_centrality',\n",
       " 'relative_cosine_similarity',\n",
       " 'resize_vectors',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'set_vecattr',\n",
       " 'similar_by_key',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_unseen_docs',\n",
       " 'sort_by_descending_frequency',\n",
       " 'unit_normalize_all',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-honey",
   "metadata": {},
   "source": [
    "> 해당 패키지는 요구된 형식으로 저장된 txt를 불러오면 우리가 원하는 token에 맞게 word vector를 반환해줄 수 있는 NLP 전용 패키지로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "monetary-stereo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disappoint', 0.9425361752510071),\n",
       " ('anime', 0.9252254366874695),\n",
       " ('ages', 0.9018864035606384),\n",
       " ('networks', 0.8915086388587952),\n",
       " ('2001', 0.887840986251831),\n",
       " ('rewarded', 0.8859958052635193),\n",
       " ('youthful', 0.8825443387031555),\n",
       " ('freaks', 0.8819042444229126),\n",
       " ('groundbreaking', 0.8807623386383057),\n",
       " ('cheers', 0.8783416152000427)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 재미있는 실험도 할 수 있다.\n",
    "# 이런 식으로 비슷한 단어라고 컴퓨터가 뽑아내는 단어들을 볼 수도 있다.\n",
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-translation",
   "metadata": {},
   "source": [
    "> 얼핏 봐도 'love'랑 많이 차이가 난다.  \n",
    "> 이는 우리의 데이터가 그만큼 워드벡터를 훈련시키기에 정교하지 않거나 수가 적거나 하다는 뜻.  \n",
    "> 이제는 구글에서 제공하는 Word2Vec이라는 pre-trained 된 워드 임베딩 모델 사용  \n",
    "> 1억 개의 단어로 구성된 Google News Dataset을 바탕으로 학습했고,  \n",
    "> 총 300만 개의 단어를 300차원으로 표현했음  \n",
    "> pre-trained된 임베딩 모델을 쓰는 게 왜 좋은지에 대한 글(https://ratsgo.github.io/natural%20language%20processing/2019/09/12/embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-rover",
   "metadata": {},
   "source": [
    "> 데이터는 이미 LMS에서는 다운로드 되어 있다.(심볼릭 링크로 연결했음)  \n",
    "> 추후에는 내가 다운받아야 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "satellite-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "# 300만 개 다 가져오면 메모리 에러가 날 수 있어서 limit을 100만개로 둠\n",
    "\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "south-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907791495323181),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100708842277527),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547304749488831),\n",
       " ('absolutely_adore', 0.5536840558052063),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-trout",
   "metadata": {},
   "source": [
    "> 이제 윗 임베딩 레이어에서 썼던 워드벡터에서 각 단어에 맞는 벡터 값을 지금 가져온 word2vec 벡터로 바꾼 행렬을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cultural-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "urban-university",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81109798, 0.37128923, 0.92756274, ..., 0.62781147, 0.06500943,\n",
       "        0.16883012],\n",
       "       [0.03807763, 0.25055212, 0.14058922, ..., 0.73076174, 0.496779  ,\n",
       "        0.27756581],\n",
       "       [0.69890595, 0.62222886, 0.15724609, ..., 0.49042867, 0.99515557,\n",
       "        0.80110967],\n",
       "       [0.92345638, 0.58310481, 0.48136944, ..., 0.36779451, 0.41038374,\n",
       "        0.2216601 ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "homeless-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 \n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "entertaining-california",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 17s 492ms/step - loss: 0.6881 - accuracy: 0.5381 - val_loss: 0.6488 - val_accuracy: 0.6265\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 12s 402ms/step - loss: 0.6133 - accuracy: 0.6676 - val_loss: 0.5003 - val_accuracy: 0.7887\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 12s 393ms/step - loss: 0.4251 - accuracy: 0.8329 - val_loss: 0.4077 - val_accuracy: 0.8141\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 13s 420ms/step - loss: 0.2911 - accuracy: 0.8838 - val_loss: 0.3340 - val_accuracy: 0.8567\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 13s 434ms/step - loss: 0.2032 - accuracy: 0.9291 - val_loss: 0.3035 - val_accuracy: 0.8726\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 13s 432ms/step - loss: 0.1529 - accuracy: 0.9513 - val_loss: 0.3035 - val_accuracy: 0.8728\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 11s 373ms/step - loss: 0.1116 - accuracy: 0.9707 - val_loss: 0.3631 - val_accuracy: 0.8540\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 12s 416ms/step - loss: 0.0896 - accuracy: 0.9759 - val_loss: 0.3451 - val_accuracy: 0.8619\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 12s 389ms/step - loss: 0.0591 - accuracy: 0.9903 - val_loss: 0.3337 - val_accuracy: 0.8740\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.0371 - accuracy: 0.9972 - val_loss: 0.3473 - val_accuracy: 0.8740\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 13s 430ms/step - loss: 0.0257 - accuracy: 0.9983 - val_loss: 0.3638 - val_accuracy: 0.8748\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 13s 432ms/step - loss: 0.0187 - accuracy: 0.9990 - val_loss: 0.3787 - val_accuracy: 0.8730\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 13s 433ms/step - loss: 0.0147 - accuracy: 0.9994 - val_loss: 0.3925 - val_accuracy: 0.8730\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 13s 432ms/step - loss: 0.0119 - accuracy: 0.9995 - val_loss: 0.4080 - val_accuracy: 0.8709\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 13s 441ms/step - loss: 0.0085 - accuracy: 0.9998 - val_loss: 0.4258 - val_accuracy: 0.8696\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 13s 442ms/step - loss: 0.0077 - accuracy: 0.9995 - val_loss: 0.4282 - val_accuracy: 0.8701\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 13s 440ms/step - loss: 0.0052 - accuracy: 0.9999 - val_loss: 0.4416 - val_accuracy: 0.8703\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 13s 440ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.4535 - val_accuracy: 0.8698\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 12s 388ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4617 - val_accuracy: 0.8702\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 11s 376ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4705 - val_accuracy: 0.8698\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train_post,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val_post, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "continued-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 8s - loss: 0.4965 - accuracy: 0.8631\n",
      "[0.49650734663009644, 0.8630800247192383]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test_post,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-chess",
   "metadata": {},
   "source": [
    "## # 결론\n",
    "> 위에서는 accuracy: 0.7184 이었던 점으로 보아 wordvector가 유의미하게 잘 구성되면 정확도가 높아간다.  \n",
    "> 이 경우엔 심지어 약 15%가 올라갔다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
